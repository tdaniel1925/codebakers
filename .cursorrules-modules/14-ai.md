# AI INTEGRATION
# Module: 14-ai.md
# Load with: 00-core.md
# Covers: OpenAI, Anthropic, Embeddings, RAG, Streaming, Chatbots

---

## ü§ñ AI CLIENT SETUP

### OpenAI Client

```typescript
// lib/ai/openai.ts
import OpenAI from 'openai';

// Validate environment
const requiredEnvVars = ['OPENAI_API_KEY'] as const;
for (const envVar of requiredEnvVars) {
  if (!process.env[envVar]) {
    throw new Error(`Missing required environment variable: ${envVar}`);
  }
}

export const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

// Default models
export const MODELS = {
  GPT4: 'gpt-4-turbo-preview',
  GPT4_VISION: 'gpt-4-vision-preview',
  GPT35: 'gpt-3.5-turbo',
  EMBEDDING: 'text-embedding-3-small',
  EMBEDDING_LARGE: 'text-embedding-3-large',
} as const;
```

### Anthropic Client

```typescript
// lib/ai/anthropic.ts
import Anthropic from '@anthropic-ai/sdk';

const requiredEnvVars = ['ANTHROPIC_API_KEY'] as const;
for (const envVar of requiredEnvVars) {
  if (!process.env[envVar]) {
    throw new Error(`Missing required environment variable: ${envVar}`);
  }
}

export const anthropic = new Anthropic({
  apiKey: process.env.ANTHROPIC_API_KEY,
});

export const CLAUDE_MODELS = {
  OPUS: 'claude-3-opus-20240229',
  SONNET: 'claude-3-sonnet-20240229',
  HAIKU: 'claude-3-haiku-20240307',
} as const;
```

---

## üí¨ CHAT COMPLETIONS

### Basic Chat (OpenAI)

```typescript
// services/ai/chat.ts
import { openai, MODELS } from '@/lib/ai/openai';

interface ChatMessage {
  role: 'system' | 'user' | 'assistant';
  content: string;
}

interface ChatOptions {
  model?: keyof typeof MODELS;
  temperature?: number;
  maxTokens?: number;
  systemPrompt?: string;
}

export async function chat(
  messages: ChatMessage[],
  options: ChatOptions = {}
): Promise<string> {
  const {
    model = 'GPT4',
    temperature = 0.7,
    maxTokens = 4096,
    systemPrompt,
  } = options;

  const allMessages: ChatMessage[] = systemPrompt
    ? [{ role: 'system', content: systemPrompt }, ...messages]
    : messages;

  const response = await openai.chat.completions.create({
    model: MODELS[model],
    messages: allMessages,
    temperature,
    max_tokens: maxTokens,
  });

  return response.choices[0]?.message?.content || '';
}
```

### Basic Chat (Anthropic)

```typescript
// services/ai/claude-chat.ts
import { anthropic, CLAUDE_MODELS } from '@/lib/ai/anthropic';

interface ChatOptions {
  model?: keyof typeof CLAUDE_MODELS;
  temperature?: number;
  maxTokens?: number;
  systemPrompt?: string;
}

export async function claudeChat(
  userMessage: string,
  options: ChatOptions = {}
): Promise<string> {
  const {
    model = 'SONNET',
    temperature = 0.7,
    maxTokens = 4096,
    systemPrompt,
  } = options;

  const response = await anthropic.messages.create({
    model: CLAUDE_MODELS[model],
    max_tokens: maxTokens,
    system: systemPrompt,
    messages: [{ role: 'user', content: userMessage }],
  });

  const textBlock = response.content.find((block) => block.type === 'text');
  return textBlock?.text || '';
}
```

---

## üåä STREAMING RESPONSES

### Streaming Chat (OpenAI)

```typescript
// services/ai/stream.ts
import { openai, MODELS } from '@/lib/ai/openai';

export async function* streamChat(
  messages: { role: 'system' | 'user' | 'assistant'; content: string }[],
  options: { model?: keyof typeof MODELS; temperature?: number } = {}
): AsyncGenerator<string> {
  const { model = 'GPT4', temperature = 0.7 } = options;

  const stream = await openai.chat.completions.create({
    model: MODELS[model],
    messages,
    temperature,
    stream: true,
  });

  for await (const chunk of stream) {
    const content = chunk.choices[0]?.delta?.content;
    if (content) {
      yield content;
    }
  }
}
```

### Streaming API Route (Next.js)

```typescript
// app/api/chat/route.ts
import { openai, MODELS } from '@/lib/ai/openai';
import { NextRequest } from 'next/server';

export async function POST(request: NextRequest) {
  const { messages, systemPrompt } = await request.json();

  const allMessages = systemPrompt
    ? [{ role: 'system' as const, content: systemPrompt }, ...messages]
    : messages;

  const stream = await openai.chat.completions.create({
    model: MODELS.GPT4,
    messages: allMessages,
    stream: true,
  });

  // Create a readable stream
  const encoder = new TextEncoder();
  const readable = new ReadableStream({
    async start(controller) {
      for await (const chunk of stream) {
        const content = chunk.choices[0]?.delta?.content;
        if (content) {
          controller.enqueue(encoder.encode(content));
        }
      }
      controller.close();
    },
  });

  return new Response(readable, {
    headers: {
      'Content-Type': 'text/plain; charset=utf-8',
      'Transfer-Encoding': 'chunked',
    },
  });
}
```

### Client-Side Streaming Hook

```typescript
// hooks/useStreamingChat.ts
'use client';

import { useState, useCallback } from 'react';

interface Message {
  role: 'user' | 'assistant';
  content: string;
}

export function useStreamingChat() {
  const [messages, setMessages] = useState<Message[]>([]);
  const [isLoading, setIsLoading] = useState(false);
  const [error, setError] = useState<string | null>(null);

  const sendMessage = useCallback(async (content: string) => {
    setIsLoading(true);
    setError(null);

    const userMessage: Message = { role: 'user', content };
    const newMessages = [...messages, userMessage];
    setMessages(newMessages);

    try {
      const response = await fetch('/api/chat', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ messages: newMessages }),
      });

      if (!response.ok) throw new Error('Failed to send message');
      if (!response.body) throw new Error('No response body');

      const reader = response.body.getReader();
      const decoder = new TextDecoder();
      let assistantContent = '';

      // Add empty assistant message
      setMessages((prev) => [...prev, { role: 'assistant', content: '' }]);

      while (true) {
        const { done, value } = await reader.read();
        if (done) break;

        assistantContent += decoder.decode(value);
        setMessages((prev) => [
          ...prev.slice(0, -1),
          { role: 'assistant', content: assistantContent },
        ]);
      }
    } catch (err) {
      setError(err instanceof Error ? err.message : 'Unknown error');
    } finally {
      setIsLoading(false);
    }
  }, [messages]);

  const clearMessages = useCallback(() => {
    setMessages([]);
  }, []);

  return { messages, isLoading, error, sendMessage, clearMessages };
}
```

---

## üìê EMBEDDINGS

### Generate Embeddings

```typescript
// services/ai/embeddings.ts
import { openai, MODELS } from '@/lib/ai/openai';

export async function generateEmbedding(text: string): Promise<number[]> {
  const response = await openai.embeddings.create({
    model: MODELS.EMBEDDING,
    input: text,
  });

  return response.data[0].embedding;
}

export async function generateEmbeddings(texts: string[]): Promise<number[][]> {
  const response = await openai.embeddings.create({
    model: MODELS.EMBEDDING,
    input: texts,
  });

  return response.data.map((item) => item.embedding);
}
```

### Cosine Similarity

```typescript
// lib/ai/similarity.ts
export function cosineSimilarity(a: number[], b: number[]): number {
  if (a.length !== b.length) {
    throw new Error('Vectors must have the same length');
  }

  let dotProduct = 0;
  let normA = 0;
  let normB = 0;

  for (let i = 0; i < a.length; i++) {
    dotProduct += a[i] * b[i];
    normA += a[i] * a[i];
    normB += b[i] * b[i];
  }

  return dotProduct / (Math.sqrt(normA) * Math.sqrt(normB));
}

export function findMostSimilar(
  query: number[],
  documents: { id: string; embedding: number[] }[],
  topK: number = 5
): { id: string; score: number }[] {
  const scored = documents.map((doc) => ({
    id: doc.id,
    score: cosineSimilarity(query, doc.embedding),
  }));

  return scored.sort((a, b) => b.score - a.score).slice(0, topK);
}
```

---

## üîç RAG (Retrieval-Augmented Generation)

### Vector Store with Supabase pgvector

```typescript
// services/ai/vector-store.ts
import { supabase } from '@/lib/supabase';
import { generateEmbedding } from './embeddings';

interface Document {
  id: string;
  content: string;
  metadata?: Record<string, unknown>;
}

export async function addDocument(doc: Document): Promise<void> {
  const embedding = await generateEmbedding(doc.content);

  await supabase.from('documents').insert({
    id: doc.id,
    content: doc.content,
    metadata: doc.metadata || {},
    embedding,
  });
}

export async function searchDocuments(
  query: string,
  limit: number = 5
): Promise<Document[]> {
  const queryEmbedding = await generateEmbedding(query);

  const { data, error } = await supabase.rpc('match_documents', {
    query_embedding: queryEmbedding,
    match_count: limit,
  });

  if (error) throw error;
  return data;
}
```

### Supabase pgvector Setup

```sql
-- Enable the pgvector extension
create extension if not exists vector;

-- Create documents table
create table documents (
  id uuid primary key default gen_random_uuid(),
  content text not null,
  metadata jsonb default '{}',
  embedding vector(1536),
  created_at timestamptz default now()
);

-- Create similarity search function
create or replace function match_documents(
  query_embedding vector(1536),
  match_count int default 5
)
returns table (
  id uuid,
  content text,
  metadata jsonb,
  similarity float
)
language plpgsql
as $$
begin
  return query
  select
    documents.id,
    documents.content,
    documents.metadata,
    1 - (documents.embedding <=> query_embedding) as similarity
  from documents
  order by documents.embedding <=> query_embedding
  limit match_count;
end;
$$;

-- Create index for faster searches
create index on documents using ivfflat (embedding vector_cosine_ops)
with (lists = 100);
```

### RAG Chat

```typescript
// services/ai/rag-chat.ts
import { chat } from './chat';
import { searchDocuments } from './vector-store';

export async function ragChat(
  userMessage: string,
  options: { systemPrompt?: string; topK?: number } = {}
): Promise<string> {
  const { systemPrompt, topK = 5 } = options;

  // Search for relevant documents
  const relevantDocs = await searchDocuments(userMessage, topK);

  // Build context from documents
  const context = relevantDocs
    .map((doc) => doc.content)
    .join('\n\n---\n\n');

  // Build RAG prompt
  const ragSystemPrompt = `${systemPrompt || 'You are a helpful assistant.'}

Use the following context to answer the user's question. If the context doesn't contain relevant information, say so.

Context:
${context}`;

  return chat(
    [{ role: 'user', content: userMessage }],
    { systemPrompt: ragSystemPrompt }
  );
}
```

---

## üõ†Ô∏è FUNCTION CALLING / TOOLS

### OpenAI Function Calling

```typescript
// services/ai/function-calling.ts
import { openai, MODELS } from '@/lib/ai/openai';
import type { ChatCompletionTool } from 'openai/resources/chat/completions';

// Define your tools
const tools: ChatCompletionTool[] = [
  {
    type: 'function',
    function: {
      name: 'get_weather',
      description: 'Get the current weather in a location',
      parameters: {
        type: 'object',
        properties: {
          location: {
            type: 'string',
            description: 'The city and state, e.g. San Francisco, CA',
          },
          unit: {
            type: 'string',
            enum: ['celsius', 'fahrenheit'],
          },
        },
        required: ['location'],
      },
    },
  },
  {
    type: 'function',
    function: {
      name: 'search_database',
      description: 'Search the database for information',
      parameters: {
        type: 'object',
        properties: {
          query: {
            type: 'string',
            description: 'The search query',
          },
          limit: {
            type: 'number',
            description: 'Maximum number of results',
          },
        },
        required: ['query'],
      },
    },
  },
];

// Tool implementations
const toolImplementations: Record<string, (args: unknown) => Promise<unknown>> = {
  get_weather: async (args: { location: string; unit?: string }) => {
    // Implement actual weather API call
    return { temperature: 72, unit: args.unit || 'fahrenheit', location: args.location };
  },
  search_database: async (args: { query: string; limit?: number }) => {
    // Implement actual database search
    return { results: [], query: args.query };
  },
};

export async function chatWithTools(
  messages: { role: 'system' | 'user' | 'assistant'; content: string }[]
): Promise<string> {
  const response = await openai.chat.completions.create({
    model: MODELS.GPT4,
    messages,
    tools,
    tool_choice: 'auto',
  });

  const message = response.choices[0].message;

  // If no tool calls, return the message content
  if (!message.tool_calls) {
    return message.content || '';
  }

  // Process tool calls
  const toolResults = await Promise.all(
    message.tool_calls.map(async (toolCall) => {
      const fn = toolImplementations[toolCall.function.name];
      if (!fn) throw new Error(`Unknown function: ${toolCall.function.name}`);

      const args = JSON.parse(toolCall.function.arguments);
      const result = await fn(args);

      return {
        tool_call_id: toolCall.id,
        role: 'tool' as const,
        content: JSON.stringify(result),
      };
    })
  );

  // Continue conversation with tool results
  const followUp = await openai.chat.completions.create({
    model: MODELS.GPT4,
    messages: [
      ...messages,
      message,
      ...toolResults,
    ],
  });

  return followUp.choices[0].message.content || '';
}
```

---

## üí∞ COST TRACKING

### Token Counting & Cost Estimation

```typescript
// lib/ai/costs.ts
import { encoding_for_model, TiktokenModel } from 'tiktoken';

// Pricing per 1K tokens (as of 2024)
const PRICING = {
  'gpt-4-turbo-preview': { input: 0.01, output: 0.03 },
  'gpt-4': { input: 0.03, output: 0.06 },
  'gpt-3.5-turbo': { input: 0.0005, output: 0.0015 },
  'text-embedding-3-small': { input: 0.00002, output: 0 },
  'claude-3-opus': { input: 0.015, output: 0.075 },
  'claude-3-sonnet': { input: 0.003, output: 0.015 },
} as const;

export function countTokens(text: string, model: TiktokenModel = 'gpt-4'): number {
  const encoder = encoding_for_model(model);
  const tokens = encoder.encode(text);
  encoder.free();
  return tokens.length;
}

export function estimateCost(
  inputTokens: number,
  outputTokens: number,
  model: keyof typeof PRICING
): number {
  const pricing = PRICING[model];
  return (inputTokens / 1000) * pricing.input + (outputTokens / 1000) * pricing.output;
}
```

### Usage Logging

```typescript
// services/ai/usage-logger.ts
import { db } from '@/lib/db';
import { aiUsageLogs } from '@/db/schema';

interface UsageLog {
  userId: string;
  model: string;
  inputTokens: number;
  outputTokens: number;
  cost: number;
  endpoint: string;
}

export async function logAiUsage(usage: UsageLog): Promise<void> {
  await db.insert(aiUsageLogs).values({
    ...usage,
    createdAt: new Date(),
  });
}

export async function getUserUsage(
  userId: string,
  startDate: Date,
  endDate: Date
): Promise<{ totalCost: number; totalTokens: number }> {
  const logs = await db
    .select()
    .from(aiUsageLogs)
    .where(
      and(
        eq(aiUsageLogs.userId, userId),
        gte(aiUsageLogs.createdAt, startDate),
        lte(aiUsageLogs.createdAt, endDate)
      )
    );

  return {
    totalCost: logs.reduce((sum, log) => sum + log.cost, 0),
    totalTokens: logs.reduce((sum, log) => sum + log.inputTokens + log.outputTokens, 0),
  };
}
```

---

## üéØ PROMPT TEMPLATES

### Prompt Management

```typescript
// lib/ai/prompts.ts
export const PROMPTS = {
  SUMMARIZE: `Summarize the following text in 2-3 sentences:

{text}`,

  EXTRACT_ENTITIES: `Extract all named entities (people, places, organizations, dates) from the following text. Return as JSON.

Text: {text}

Return format: { "people": [], "places": [], "organizations": [], "dates": [] }`,

  CLASSIFY: `Classify the following text into one of these categories: {categories}

Text: {text}

Return only the category name.`,

  TRANSLATE: `Translate the following text to {language}:

{text}`,

  CODE_REVIEW: `Review the following code and provide feedback on:
1. Bugs or potential issues
2. Performance concerns
3. Best practices
4. Suggestions for improvement

Code:
\`\`\`{language}
{code}
\`\`\``,
} as const;

export function fillPrompt(
  template: string,
  variables: Record<string, string>
): string {
  return Object.entries(variables).reduce(
    (prompt, [key, value]) => prompt.replace(new RegExp(`\\{${key}\\}`, 'g'), value),
    template
  );
}

// Usage
// const prompt = fillPrompt(PROMPTS.SUMMARIZE, { text: 'Long article...' });
```

---

## üîê SECURITY & RATE LIMITING

### AI Rate Limiting

```typescript
// middleware/ai-rate-limit.ts
import { Ratelimit } from '@upstash/ratelimit';
import { Redis } from '@upstash/redis';
import { NextRequest, NextResponse } from 'next/server';

const ratelimit = new Ratelimit({
  redis: Redis.fromEnv(),
  limiter: Ratelimit.slidingWindow(10, '1 m'), // 10 requests per minute
  analytics: true,
});

export async function aiRateLimitMiddleware(
  request: NextRequest,
  userId: string
) {
  const { success, limit, remaining, reset } = await ratelimit.limit(
    `ai:${userId}`
  );

  if (!success) {
    return NextResponse.json(
      { error: 'Rate limit exceeded. Please try again later.' },
      {
        status: 429,
        headers: {
          'X-RateLimit-Limit': limit.toString(),
          'X-RateLimit-Remaining': remaining.toString(),
          'X-RateLimit-Reset': reset.toString(),
        },
      }
    );
  }

  return null; // Continue
}
```

### Input Sanitization

```typescript
// lib/ai/sanitize.ts
export function sanitizePromptInput(input: string): string {
  // Remove potential prompt injection attempts
  const sanitized = input
    // Remove system-like instructions
    .replace(/\b(system|assistant|ignore previous|disregard|forget)\b/gi, '')
    // Limit length
    .slice(0, 10000)
    // Remove excessive whitespace
    .replace(/\s+/g, ' ')
    .trim();

  return sanitized;
}

export function validatePromptInput(input: string): { valid: boolean; error?: string } {
  if (!input || input.trim().length === 0) {
    return { valid: false, error: 'Input cannot be empty' };
  }

  if (input.length > 10000) {
    return { valid: false, error: 'Input too long (max 10000 characters)' };
  }

  // Check for obvious injection attempts
  const injectionPatterns = [
    /ignore all previous/i,
    /disregard your instructions/i,
    /you are now/i,
    /new instructions:/i,
  ];

  for (const pattern of injectionPatterns) {
    if (pattern.test(input)) {
      return { valid: false, error: 'Invalid input detected' };
    }
  }

  return { valid: true };
}
```

---

## üß™ TESTING AI FEATURES

### AI Service Tests

```typescript
// __tests__/services/ai.test.ts
import { describe, it, expect, vi } from 'vitest';
import { chat } from '@/services/ai/chat';
import { generateEmbedding } from '@/services/ai/embeddings';

// Mock OpenAI
vi.mock('@/lib/ai/openai', () => ({
  openai: {
    chat: {
      completions: {
        create: vi.fn().mockResolvedValue({
          choices: [{ message: { content: 'Mocked response' } }],
        }),
      },
    },
    embeddings: {
      create: vi.fn().mockResolvedValue({
        data: [{ embedding: new Array(1536).fill(0.1) }],
      }),
    },
  },
  MODELS: {
    GPT4: 'gpt-4-turbo-preview',
    EMBEDDING: 'text-embedding-3-small',
  },
}));

describe('AI Chat', () => {
  it('should return a response', async () => {
    const response = await chat([{ role: 'user', content: 'Hello' }]);
    expect(response).toBe('Mocked response');
  });
});

describe('Embeddings', () => {
  it('should generate embedding of correct dimension', async () => {
    const embedding = await generateEmbedding('Test text');
    expect(embedding).toHaveLength(1536);
  });
});
```

---

## üìã CHECKLIST

Before shipping AI features:

- [ ] API keys stored in environment variables
- [ ] Rate limiting implemented
- [ ] Input validation and sanitization
- [ ] Cost tracking/usage logging
- [ ] Error handling for API failures
- [ ] Streaming for long responses
- [ ] User feedback mechanism
- [ ] Content moderation (if user-generated prompts)
- [ ] Fallback for API outages
- [ ] Tests for AI service layer
